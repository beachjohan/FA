\documentclass[titlepage]{article}
% NOTE: this file is encoded in utf-8


\DeclareMathAlphabet{\mathscr}{OT1}{pzc}{m}{it}
\usepackage{subfig}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{here}
\usepackage[utf8x]{inputenc}
\usepackage{bm}
\usepackage{color}
\usepackage{amsthm}

\begin{document}
\section{Extra material}
\subsection{Jensens inequality}
Finite case: For a real convex function $\phi$
$$ \phi\left(\sum_{i= 1}^N \theta_ix_i\right) \leq \sum_{i=1}^N\theta_i \phi(x_i)$$
\begin{proof}
By induction: n = 2 follows from convexity and for $N+1$:
$$\phi\left(\sum_{i= 1}^{N+1} \theta_ix_i\right) =  \phi\left(\theta_1x_1 + (1-\theta_1)\sum_{i= 2}^{N+1} \frac{\theta_i}{1-\theta_1}x_i\right) $$
By convexity of the case $n= 2$ the result follows.
\end{proof}
Probability formulation:
Let $X$ be some random variable, and led $f(x)$ be a convex function (defined at least on a segment containing the rainge of $X$). Then:

$$\mathbb{E}[f(x)] \geq f(\mathbb{E}[X])$$
\begin{proof}
Let $c = \mathbb{E}[X]$. Since $f(x)$ is convex there exists a supporting line for $f(x)$ at c:
$$\phi(x) = \alpha(x-c) + f(x)$$
for some $\alpha$, and $\phi(x) \leq f(x)$. Then
$$ \mathbb{E}[f(X)] \geq \mathbb{E[\phi(X)]} = \mathbb{E}[\alpha(X-c) + f(c)] = f(c)$$
\end{proof}
\subsection{Youngs Inequality}
Let $a,b ,p, q \in \mathbb{R}_{++}$ such that
$$\frac{1}{p} + \frac{1}{q} = 1$$
then:
$$ab\leq \frac{a^p}{p}+ \frac{b^q}{q} $$
\begin{proof}
$$ab = exp\left(\frac{1}{p} ln(a^p) + \frac{1}{q}ln(b^q)\right) \leq 
\frac{1}{p}exp(ln(a^p)) + \frac{1}{q}exp(ln(b^q)) = \frac{a^p}{p} + \frac{b^q}{q} $$
where the inequality follows from the convexity of the exponential function,
\end{proof}

\subsubsection{Standard version}:
Let $f:[0,c]\longrightarrow R$ be a continuous and strictly increasing function, where $f(0) = 0$.
$$ab\leq\int_0^a f(x)ds + \int_0^b f^{-1}(x)dx $$
\begin{proof}
By visual inspection.
\end{proof}
\subsection{Hölders inequality}
\subsubsection{Standard version}
Let $(S,\sum, \mu)$ be a measure space and let $q \leq p, q \leq \infty$ with $\frac{1}{p}+ \frac{1}{q} = 1$ then for all measurable real- or conplex-valued functions $f$ and $g$ on $S$
$$\|fg\|_1 \leq \|f\|_p \|g\|_q,$$
in the case $p = 2$ this becomes the Cauchy inequality.
\begin{proof}
if $\|f\|_{t} = 0$ for $t = p$ or $t = q$ the left side is zero almost everywhere, so we can assume that $\|f|\|_t > 0$. For $\|f\|_{t}= \infty$ the right hand side could be infinite large. 
if $t = \infty$ then $|fg|\leq\|f\|_\infty|g|$ almost everywhere and Hölders inequality follows from the monotonicity of the Lebesque integral (?). Normalizing the equation we can expect
$$\|f\|_p = \|g\|_q = 1.$$
Using Youngs inequality
$$|f(s) g(s)| \leq \frac{|f(s)|^p}{p} + \frac{|g(s)| ^q}{q} \; s\in S$$
Integrating both sides:
$$\|fg\|_1 = \int_S |f(s)g(s)| ds \leq \int_s \left(\frac{|f(s)|^p}{p} + \frac{|g(s)| ^q}{q} ds\right) = \frac{\|f\|_p}{p} + \frac{\|g\|_q}{q} = 1 = \|f\|_p \|g\|_q $$
\end{proof}
\subsubsection{With sums}
Using summation to define the norm :
$$\|x\|_p = \left(\sum_k|x_k|^p\right)^{\frac{1}{p}}$$
\begin{proof}


then (without normalization)
$$\frac{|\sum_kx_ky_k|}{\|x\|_p\|x\|_q} \leq  \sum_k\frac{|x_k||y_k|}{\|x\|_p\|x\|_q} \leq  \frac{1}{p}\sum \frac{|x_k|^p}{\|x\|_p^p}+ \frac{1}{p}\sum \frac{|x_k|^q}{\|x\|_q^q} = \frac{1}{p} + \frac{1}{q} = 1$$
hence
$$|(x,y)||= |\sum_kx_ky_k| \leq {\|x\|_p\|x\|_q}$$
\end{proof}
The equality holds (proof) iff $arg x_iy_i$ and $|x_j|^p|y_j|^p$ are independent of j. For any $x\in \ell^p$ we can choose $y\in \ell^q$ so we get equality (proof?). Hence we get 
$$ \|x\|_p = \max\limits_{\substack{\|u\|_q = 1}}\|(xu)\|_1$$

\subsection{Minkowski}
For the $\ell^p$ spaces (same holds for $L^p$)
$$\|x+p\|_p \leq\|x\|_p+ \|y\|_p$$
\begin{proof}
(?)
Using the second formulation of hölder inequality and noting that $\|xy\|_1$ could be written as a scalar product and that scalar products are bilinear
$$\|x+y\|_p = \max\limits_{\substack{\|u\|_q = 1}}|(x+y,u)| \leq
\max\limits_{\substack{\|u\|_q = 1}}|(x,u)|+ |(y,u)| \leq \|x\|_p +\|y\|_q.$$
Where we in the last inequality splitt the max and use Hölder inequality together with fact that $\|u\|_q = 1.$
\end{proof}
\subsection{Cauchy inequality}
A scalar product in a linear space $X$ over  $\mathbb{C}$ is a real valued function that fulfils 
\begin{itemize}
\item sesquilinearity: $(ax,y) = a(x,y),\;\; (x,ay) = \bar{a}(x,y)$ 
\item Skew symmetry: $(y,x) =  \overline{(x,y)}$ 
\item positivity: $(x,x) > 0,  \;\; x\neq 0$ 
\end{itemize}
This also lets us define the norm  
$$\|x\| = (x,x)^{\frac{1}{2}}$$

\subsubsection{Schwartz Inequality in $\mathbb{C}$}
$$|(x,y)| \leq \|x\|\|y\|$$
(This is just a special case of hölders inequality with $p= 2$)
\begin{proof}
For any $t \in \mathbb{R}$ and $y\neq 0$
\begin{equation}
0\leq \|x+ty\|^2 = \|x\|^2 + 2tRe(x,y) + t^2\|y\|^2 
\label{scalar}
\end{equation}
By choosing $t = -Re(x,y)/\|y\|^2$ and multiplying with $\|y\|^2$
\begin{equation}
\left(Re(x,y)\right)^2 \leq \|x\|^2\|y\|^2
\end{equation}
Let $x = ax, a\in \mathbb{C}$ such that $|a| = 1$ and $Im(a(x,y)) = 0$
results in
\begin{equation}
|(x,y)| \leq \|x\|\|y\|
\end{equation} 

This also gives as a corollary that every vector in a scalar product space can be written as 
$$ \|x\| = \max\limits_{\substack{\|y\| = 1}}\|(x,y)\|_1$$

\end{proof}
\subsection{Parallelogram Identity}
Set $t\pm 1$ in \eqref{scalar} and we end up with the parallelogram identity, which is true for all scalar product induced norms.
\begin{equation}
\label{parallel}
\|x+y\|^2 + \|x-y\|^2 = 2\left(\|x\|^2+ \|y\|^2 \right)
\end{equation}
Actually every norm that comes from an scalar product must have this equality (von Neumann)
\subsection{Polarization Identity}
For a real vector space, if the norm fullfills the Parallelogram Identity then it induces a scalar product
\begin{equation}
(x,y) = \frac{1}{4}\left(\|x+y\|^2 - \|x-y\|^2\right)
\label{polreel}
\end{equation}
And for the complex vector space
\begin{equation}
(x,y) = \frac{1}{4}\left(\|x+y\|^2 - \|x-y\|^2 +i\|x+iy\|^2 - i\|x-iy\|^2   \right)
\label{polcomp}
\end{equation}
\section{Finite}
\subsubsection{Lemma x}
Let $Y$ be a subset of the metric space $X$, then $x\in X$ is adherent to Y iff there is a sequence in Y that converges to x.
\begin{proof}
\title{lemma x}
$(\Rightarrow)$ For each $n\geq 1$ there exists some point $x_n \in B(x;1/n)\cap Y$ the sequence $(x_n)$ then converges to $x$.

$(\Leftarrow)$  If there is a sequence $(x_n)$ converging to $x$ then all balls centered at $x$ contains points from the sequence.
\end{proof}
\subsubsection{Every subspace of a normed space of finite dimension is closed}
Let $(V, |\cdot|$) be a normed vector space of finite dimension, and $S\subset V$ a finite dimensional subspace. Then S is closed.
\begin{proof}
Let $a\in \overline{S}$ and choose a sequence $(a_n)$, with $a_n \in S$ converging to $a$ \ref{lemma x}. Then $(a_n)$ is a cauchy sequence in $V$ and also a cauchy sequence in S. Since $V$ is finite dimensional, $V$ is a Banach space, $S$ is complete, so $(a_n)$ converges to an element in $S$. Since limits in a normed space are unique, that limits must be $a$, so $a\in S$.
\end{proof}
\subsubsection{Every finite dimensional subspace of a normed space is closed}

\begin{proof}
Let $(V, |\cdot|$) be a normed vector space, and $S\subset V$ a finite dimensional subspace. Then S is closed.
Let $x\in V$ and let $(a_n)$ be a sequence in $S$ with converges to $x$. Because $S$ is of finite dimension, we have a basis $\{x_1...x_k$ of$S$. Also $x\in span(x_1...x_k,x)$. But as proved in the case when V is finite dimensional, we have that $S$ is closed in $span(x_1...x_k,x$ (taken with the normed induced by $(V, |\cdot|)$) with $a_n \rightarrow x$, and $x\in S$.
\end{proof}
något att ändra2
\end{document}
