\documentclass[titlepage]{article}
% NOTE: this file is encoded in utf-8


\DeclareMathAlphabet{\mathscr}{OT1}{pzc}{m}{it}
\usepackage{subfig}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
% \usepackage{here}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{color}
\usepackage{amsthm}
%\newcommand{limunder}[1]{\limits_{\substack{#1}}}


% Select what to do with todonotes: 
% \usepackage[disable]{todonotes} % notes not showed
\usepackage[draft]{todonotes}   % notes showed

\begin{document}
\section{Extra material}
\subsection{Jensens inequality}
Finite case: For a real convex function $\phi$
$$\phi\left(\sum_{i= 1}^N \theta_ix_i\right) \leq \sum_{i=1}^N\theta_i \phi(x_i)$$
\begin{proof}
By induction: n = 2 follows from convexity and for $N+1$:
$$\phi\left(\sum_{i= 1}^{N+1} \theta_ix_i\right) =  \phi\left(\theta_1x_1 + (1-\theta_1)\sum_{i= 2}^{N+1} \frac{\theta_i}{1-\theta_1}x_i\right) $$
By convexity of the case $n= 2$ the result follows.
\end{proof}
Probability formulation:
Let $X$ be some random variable, and let $f(x)$ be a convex function (defined at least on a segment containing the range of $X$). Then:

$$\mathbb{E}[f(x)] \geq f(\mathbb{E}[X])$$
\begin{proof}
Let $c = \mathbb{E}[X]$. Since $f(x)$ is convex there exists a supporting line for $f(x)$ at c:
$$\phi(x) = \alpha(x-c) + f(x)$$
for some $\alpha$, and $\phi(x) \leq f(x)$. Then
$$ \mathbb{E}[f(X)] \geq \mathbb{E[\phi(X)]} = \mathbb{E}[\alpha(X-c) + f(c)] = f(c)$$
\end{proof}
\subsection{Youngs Inequality}
Let $a,b ,p, q \in \mathbb{R}_{++}$ such that
$$\frac{1}{p} + \frac{1}{q} = 1$$
then:
$$ab\leq \frac{a^p}{p}+ \frac{b^q}{q} $$
\begin{proof}
$$ab = \exp\left(\frac{1}{p} \ln(a^p) + \frac{1}{q}\ln(b^q)\right) \leq 
\frac{1}{p}\exp(\ln(a^p)) + \frac{1}{q}\exp(\ln(b^q)) = \frac{a^p}{p} + \frac{b^q}{q} $$
where the inequality follows from the convexity of the exponential function.
\end{proof}

\subsubsection{Standard version}
Let $f:[0,c]\longrightarrow R$ be a continuous and strictly increasing function, where $f(0) = 0$.
$$ab\leq\int_0^a f(x)dx + \int_0^b f^{-1}(x)dx $$

\begin{proof}
By visual inspection.
\end{proof}
\subsection{Hölders inequality}
\subsubsection{Standard version}
Let $(S,\sum, \mu)$ be a measure space and let $q \leq p, q \leq \infty$ with $\frac{1}{p}+ \frac{1}{q} = 1$ then for all measurable real- or complex-valued functions $f$ and $g$ on $S$
$$\|fg\|_1 \leq \|f\|_p \|g\|_q,$$
in the case $p = 2$ this becomes the Cauchy inequality.
\begin{proof}
If $\|f\|_{t} = 0$ for $t = p$ or $t = q$ the left side is zero almost everywhere, so we can assume that $\|f\|_t > 0$. For $\|f\|_{t}= \infty$ the right hand side could be infinite large. 
If $t = \infty$ then $|fg|\leq\|f\|_\infty|g|$ almost everywhere and Hölders inequality follows from the monotonicity of the Lebesque integral (?). Normalizing the equation we can expect
$$\|f\|_p = \|g\|_q = 1.$$
Using Youngs inequality
$$|f(s) g(s)| \leq \frac{|f(s)|^p}{p} + \frac{|g(s)| ^q}{q} \; s\in S$$
Integrating both sides:
$$\|fg\|_1 = \int_S |f(s)g(s)| ds \leq \int_s \left(\frac{|f(s)|^p}{p} + \frac{|g(s)| ^q}{q} \right) ds = \frac{\|f\|_p}{p} + \frac{\|g\|_q}{q} = 1 = \|f\|_p \|g\|_q $$
\end{proof}
\subsubsection{With sums}
Using summation to define the norm :
$$\|x\|_p = \left(\sum_k|x_k|^p\right)^{\frac{1}{p}}$$
\begin{proof}


Then (without normalization)
$$\frac{|\sum_kx_ky_k|}{\|x\|_p\|x\|_q} \leq  \sum_k\frac{|x_k||y_k|}{\|x\|_p\|x\|_q} \leq  \frac{1}{p}\sum \frac{|x_k|^p}{\|x\|_p^p}+ \frac{1}{p}\sum \frac{|x_k|^q}{\|x\|_q^q} = \frac{1}{p} + \frac{1}{q} = 1$$
hence
$$|(x,y)|= |\sum_kx_ky_k| \leq {\|x\|_p\|x\|_q}$$
\end{proof}
The equality holds (proof) iff $\arg x_jy_j$  and $|x_j|^p|y_j|^p$ are independent of $j$. For any $x\in \ell^p$ we can choose $y\in \ell^q$ so we get equality (proof?). Hence we get 
$$ \|x\|_p = \max\limits_{\substack{\|y\|_q = 1}}\|(x,y)\|_1$$

\subsection{Minkowski}
For the $\ell^p$ spaces (same holds for $L^p$)
$$\|x+p\|_p \leq\|x\|_p+ \|y\|_p$$
\begin{proof}
(?)
Using the second formulation of Hölders inequality and noting that $\|xy\|_1$ could be written as a scalar product and that scalar products are bilinear
$$\|x+y\|_p = \max\limits_{\substack{\|u\|_q = 1}}|(x+y,u)| \leq
\max\limits_{\substack{\|u\|_q = 1}}|(x,u)|+ |(y,u)| \leq \|x\|_p +\|y\|_q.$$
Where we in the last inequality split the max and use Hölder inequality together with fact that $\|u\|_q = 1.$
\end{proof}
\subsection{Cauchy inequality}
A scalar product in a linear space $X$ over  $\mathbb{C}$ is a real valued function that fulfils 
\begin{itemize}
\item sesquilinearity: $(ax,y) = a(x,y),\;\; (x,ay) = \bar{a}(x,y)$ 
\item Skew symmetry: $(y,x) =  \overline{(x,y)}$ 
\item positivity: $(x,x) > 0,  \;\; x\neq 0$ 
\end{itemize}
This also lets us define the norm  
$$\|x\| = (x,x)^{\frac{1}{2}}$$

\subsubsection{Schwarz Inequality in $\mathbb{C}$}
$$|(x,y)| \leq \|x\|\|y\|$$
(This is just a special case of Hölders inequality with $p= 2$)
\begin{proof}
For any $t \in \mathbb{R}$ and $y\neq 0$
\begin{equation}
0\leq \|x+ty\|^2 = \|x\|^2 + 2tRe(x,y) + t^2\|y\|^2 
\label{scalar}
\end{equation}
By choosing $t = -Re(x,y)/\|y\|^2$ and multiplying with $\|y\|^2$
\begin{equation}
\left(Re(x,y)\right)^2 \leq \|x\|^2\|y\|^2
\end{equation}
Let $x = ax, a\in \mathbb{C}$ such that $|a| = 1$ and $Im(a(x,y)) = 0$
results in
\begin{equation}
|(x,y)| \leq \|x\|\|y\|
\end{equation} 

This also gives as a corollary that every vector in a scalar product space can be written as 
$$ \|x\| = \max\limits_{\substack{\|y\| = 1}}\|(x,y)\|_1$$

\end{proof}
\subsection{Parallelogram Identity}
Set $t\pm 1$ in \eqref{scalar} and we end up with the parallelogram identity, which is true for all scalar product induced norms.
\begin{equation}
\label{parallel}
\|x+y\|^2 + \|x-y\|^2 = 2\left(\|x\|^2+ \|y\|^2 \right)
\end{equation}
Actually every norm that comes from an scalar product must have this equality (von Neumann)
\subsection{Polarization Identity}
For a real vector space, if the norm fullfills the Parallelogram Identity then it induces a scalar product
\begin{equation}
(x,y) = \frac{1}{4}\left(\|x+y\|^2 - \|x-y\|^2\right)
\label{polreel}
\end{equation}
And for the complex vector space
\begin{equation}
(x,y) = \frac{1}{4}\left(\|x+y\|^2 - \|x-y\|^2 +i\|x+iy\|^2 - i\|x-iy\|^2   \right)
\label{polcomp}
\end{equation}
\section{Finite}
\subsubsection{Lemma x}
Let $Y$ be a subset of the metric space $X$, then $x\in X$ is adherent to Y iff there is a sequence in Y that converges to x.
\begin{proof}
\title{lemma x}
$(\Rightarrow)$ For each $n\geq 1$ there exists some point $x_n \in B(x;1/n)\cap Y$ the sequence $(x_n)$ then converges to $x$.

$(\Leftarrow)$  If there is a sequence $(x_n)$ converging to $x$ then all balls centered at $x$ contains points from the sequence.
\end{proof}
\subsubsection{Every subspace of a normed space of finite dimension is closed}
Let $(V, |\cdot|$) be a normed vector space of finite dimension, and $S\subset V$ a finite dimensional subspace. Then S is closed.
\begin{proof}
Let $a\in \overline{S}$ and choose a sequence $(a_n)$, with $a_n \in S$ converging to $a$ \ref{lemma x}. Then $(a_n)$ is a cauchy sequence in $V$ and also a cauchy sequence in S. Since $V$ is finite dimensional, $V$ is a Banach space, $S$ is complete, so $(a_n)$ converges to an element in $S$. Since limits in a normed space are unique, that limits must be $a$, so $a\in S$.
\end{proof}
\subsubsection{Every finite dimensional subspace of a normed space is closed}

\begin{proof}
Let $(V, |\cdot|$) be a normed vector space, and $S\subset V$ a finite dimensional subspace. Then S is closed.
Let $x\in V$ and let $(a_n)$ be a sequence in $S$ with converges to $x$. Because $S$ is of finite dimension, we have a basis $\{x_1...x_k$ of$S$. Also $x\in span(x_1...x_k,x)$. But as proved in the case when V is finite dimensional, we have that $S$ is closed in $span(x_1...x_k,x$ (taken with the normed induced by $(V, |\cdot|)$) with $a_n \rightarrow x$, and $x\in S$.
\end{proof}
\section{Things to remember}
\begin{enumerate}
\item \textit{Uniform convex:} For any pair of unit vectors $x,y$ the norm $|(x+y)|/2$ is strictly less then 1, by an amount that depends only on $|x-y|$.
   \begin{itemize}
        \item $L^p$ spaces are uniformly convex for $1<p<\infty$
        \item The space $C$ is not uniformly convex, not even subadditiv in the maxnorm
    \end{itemize}
\end{enumerate}

\subsubsection{Least upper bound property}
Is often taken as an axiom.
Every nonempty set $S$ of $\mathbb{R}$ must have a least upper bound (supremum).
\section{Measure theory}
\subsection*{Monotone convergence theory}
\subsubsection{Reel numbers}
Theorem: Let $(a_n)$ be a monotone sequence in $\mathbb{R}$ (i.e. $a_n \leq a_{n+1}$ or $a_n \geq a_{n+1}$ for all $n\geq1$. Then $(a_n)$ has a finite limit iff it is bounded.
\begin{proof}
(Bounded above)

$(\Leftarrow)$
By the least upper bound property there is a real number $c = \sup_n (a_n)$. For every $\epsilon \geq 0$, there exists $a_N$ such that $a_n > c-\epsilon$, otherwise $c-\epsilon$ would be an lower bound than $c$. Since $(a_n)$ is monotone $\forall n>N, |c-a_n| = c-a_n \leq c-a_N < \epsilon.$ , it is converging to $\sup(a_n)$.

($\Rightarrow$) Let $a_n \rightarrow c$ as $n\rightarrow \infty$. Then we want to find an $K$ such that $\forall n \in \mathbb{N}: |a_n|\leq K$. Since $(a_n)$ converges is it true that $$\forall \epsilon >0: \exists N: n>N \Longrightarrow |a_n - c| < \epsilon$$
Set  $\epsilon = 1$ by the reverse triangle inequality
$$\forall n >N_1 : |a_n| - |c| \leq |a_n - c| < 1,$$
hence $a_n \leq |c| + 1$.
$K = \max \left(|a_1|, |a_2, ...|a_{N_1}|, |c|+1\right)$ is finite and does the job. The monotone decreasing sequence can be done analogously.
\subsubsection{Lebesque's monotone convergence theorem}
\textit{Theorem:} Let $(X,\Sigma, \mu)$ be a measure space and $(f_1,f_2...), f_i \in \Sigma : f_i: X \rightarrow [0,\infty]$ be pointwise non-decreasing sequences, i.e
$$ 0 \leq f_k(x) \leq f_{k+1}(x).$$
Let
$$f(x) := \lim\limits_{\substack{k\rightarrow \infty}} f_k(x)$$
be the pointwise limit for $\forall x \in X.$
Then $f$ is $\Sigma-$ measurable and 
\begin{equation}
\lim\limits_{\substack{k\rightarrow \infty}}\int f_k d\mu = \int f d\mu 
\end{equation}
This can also take the shape as the sum
\begin{equation}
\lim\limits_{\substack{k\rightarrow \infty}}\sum_k a_{kj} = \sum_k \lim a_{kj}
\end{equation}
\subsubsection{Dominated convergence theorem}
 Let $f_n$ be a sequence of real-valued measurable functions on a measure space $(X,\Sigma, \mu)$, suppose the sequence converges pointwise to a function $f$ and is dominated by a integrable function $g$ in the sense that
 \begin{equation}
 |f_n(x)| \leq g(x),
 \end{equation}
 $\forall n\in \mathbb{N}$ and $x\in X$
 Then f is integrable and 
 \begin{equation}
 \lim_{n\rightarrow \infty} \int_S |f_n - f|d\mu = 0 \Longleftrightarrow
 \lim_{n\rightarrow \infty} \int_S f_n d\mu = \int_S f d\mu 
 \end{equation}
 
 \subsubsection{Fotou's Lemma}
  Let $f_n$ be a sequence of real-valued non-negative measurable functions on a measure space $(X,\Sigma, \mu)$. Define the function $f: S \rightarrow [0,\infty]$ almost everywhere by
  \begin{equation}
  f(s) = \liminf \limits_{\substack{n\rightarrow \infty}} f_n(s)\;\; s\in S
  \end{equation}
Then $f$ is measurable and 
\begin{equation}
\int_S fd\mu \leq \liminf \limits_{\substack{n\rightarrow \infty}}
\int_S f_n d\mu.
\end{equation}  
  
\section{Things we should know!}
\subsection{Bolzano - Weierstrass Theorem}
Each bounded sequence in $\mathbb{R}^n$ has a convergent subsequence. 
(or equivalently: A subset of $\mathbb{R}^n$ is sequentially compact iff it is closed and bounded)

\subsubsection{Lemma}
Every sequence $(a_n) \in \mathbb{R}$ has a monotone subsequence.
\begin{proof}

Let us call a positive integer a \textbf{Peak} of the sequence if $m>n \rightarrow a_m>a_n$. If there are infinitely many peaks we take these as an decreasing subsequence.
If we instead has finitely many peaks, suppose $N$ is the last one. Then we can find a sequence that for every value $a_{n_j}$ we have 
$n_{j+1} > n_j$ and $a_{n_j} \leq a_{n_j+1}$ (otherwise $a_{n_j}$ would be a peak. Hence we have a monotone subsequence $a_{n_1} \leq a_{n_2} \leq a_{n_3} \leq \ldots $.
\end{proof}
Continuing the proof:

$n= 1:$ Now suppose we have a bounded sequence in $\mathbb{R}$; by the previous lemma there exists a monotone subsequence, necessarily bounded. Then by monotone convergence theorem, thus subsequence converge.

$n \geq 1: $ For the bounded sequence $(a_n) \in \mathbb{R}^n$, we use a the following procedure:
let $(a_n^1)$ be a subsequence of $(a_n)$  converging in the first coordinate. Let $a_n^2$ be a subsequence of  $(a_n^1)$ converging in the first two coordinates. Continuing in this manner after $n$ subsequence we are done, and have a subsequence of $(a_n)$ that converges in every coordinate.
\end{proof}

\subsection{The Stone-Weirstrass Theorem}
\subsubsection{The original statement}
Suppose $f$ is a continuous complex-valued function defined on the real intervall $[a,b]$. For every $\epsilon>0$, there exists a polynomial function over $\mathbb{C}$ such that 
\begin{equation}
\sup_x\| f(x) - p(x)\| \leq \epsilon
\end{equation}
\subsubsection{Functional analysis}
Let $S$ be a compact Hausdorff space, $C(S)$ the set of all real-valued continuous function on $S$. Let $E$ be a subalgebra of $C(S)$, that is,
\begin{itemize}
\item $E$ is a subspace of $C(S)$.
\item The product of two functions in $E$ belongs to $E$.
\end{itemize}
In addition we impose the following condition on E:
\begin{itemize}
\item $E$ Separates points of $S$, that is, given any pair of points p and q, $p\neq q$, there is a function $f\in E$ such that $f(p) \neq f(q)$
\item The product of two functions in $E$ belongs to $E$.
\end{itemize}

\end{document}
